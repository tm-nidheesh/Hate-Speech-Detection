# Toxic-Comment-Predictor
Comment toxicity prediction is a deep learning project aimed at building a model that can automatically classify text comments as toxic or non-toxic. Toxic comments are those that contain offensive, abusive, or harassing content, and can cause harm to individuals or groups.

The goal of this project is to develop a model that can accurately predict whether a comment is toxic or non-toxic based on its text content. To achieve this, the project will involve collecting and cleaning a large dataset of comments labeled as toxic or non-toxic. 

The dataset can be collected from: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge

Once the dataset is collected and preprocessed, the next step will be to develop a deep learning model that can learn to classify the comments as toxic,severe toxic,	obscene, threat, insult or hate.
The model is build using various deep learning techniques such as bidirectional LSTM and a certain number of dense layers. The aim is to achieve high accuracy and minimize false positives and false negatives.
The trained model can be used to automatically detect toxic comments in real-time and can be integrated into various platforms such as social media platforms, online forums, and chat applications. This can help to minimize the spread of toxic comments and create a safer and more inclusive online environment.

The model has been deployed using Gradio which is an an open-source Python library that makes it easy to create and deploy custom ML interfaces using simple Python code. It can be used to build web-based interfaces for machine learning, deep learning models and can be easily integrated into various web applications.


<img width="960" alt="comment toxicity" src="https://user-images.githubusercontent.com/124424862/234604302-dbf71f5b-a6e8-4516-a6c9-497b656ecc48.png">
